{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How did we get to a single vector of activations in the CNNs used for MNIST in previous chapters? Why isn't that suitable for Imagenette?<br>\n",
    "    We used `nn.Flatten` to to convert the last conv layer to spit out the given output of given dataloaders class. For imagenette the input of size `224*224`, we have to do many convs. We will not able to optimize the network.\n",
    "1. What do we do for Imagenette instead?<br>\n",
    "    Before final Linear layer there is `nn.AdaptiveAvgPool2d(1)` which convert the activation for a unit dimensional for each channel.\n",
    "1. What is \"adaptive pooling\"?<br>\n",
    "    acts for using extract feature over given area of the input\n",
    "1. What is \"average pooling\"?<br>\n",
    "    acts for extracting mean over a given area\n",
    "1. Why do we need `Flatten` after an adaptive average pooling layer?<br>\n",
    "    to convert the `n-dim` object to unit dim which will be input to last linear layer\n",
    "1. What is a \"skip connection\"?<br>\n",
    "    accumlate the input to output after set of given operation\n",
    "1. Why do skip connections allow us to train deeper models?<br>\n",
    "    it acts as identity layer and smoothen out the loss function. Which inherently help us find the correct weight.\n",
    "1. What does <<resnet_depth>> show? How did that lead to the idea of skip connections?<br>\n",
    "    as the no of layer increase we dont overfit nor we able to lower the loss wrt the lower layer convoluted network. Inhernty the addtional layer can be thought of indntity layer which do nothing(no contribution) to the learning process.\n",
    "1. What is \"identity mapping\"?<br>\n",
    "    convert the input to output without doing any operations.\n",
    "1. What is the basic equation for a ResNet block (ignoring batchnorm and ReLU layers)?<br>\n",
    "    `x + f(f(x))` the \"f\" is considered to be convolution\n",
    "1. What do ResNets have to do with residuals?<br>\n",
    "    it predicts difference between the actual input and the prediction.\n",
    "1. How do we deal with the skip connection when there is a stride-2 convolution? How about when the number of filters changes?<br>\n",
    "    During stride-2 convolution, we use average pool of window 2*2 to down samlpe the input.\n",
    "    When the number of filters changes, we use unit conv with output feature size\n",
    "1. How can we express a 1Ã—1 convolution in terms of a vector dot product?<br>\n",
    "    **Sum notation**<br>\n",
    "    $$\n",
    "    \\displaystyle O_{k} = \\sum_{k} X_{kij} * W_{k} \n",
    "    $$\n",
    "    $X_{kij}$ : image of k features, i and j are hight and width respectively<br>\n",
    "    $W_{k}$ : weight for each channel<br>\n",
    "    **Vector Dot Product notation**\n",
    "    $$\n",
    "    \\displaystyle O_{k} = \\sum_{k} X_{kij} * W \n",
    "    $$\n",
    "    $X_{kij}$ : image of k features, i and j are hight and width respectively<br>\n",
    "    $W$ : weight matrix for all channel stacked together\n",
    "\n",
    "1. Create a `1x1 convolution` with `F.conv2d` or `nn.Conv2d` and apply it to an image. What happens to the `shape` of the image?<br>\n",
    "    it remains same\n",
    "1. What does the `noop` function return?<br>\n",
    "    no operation, can be thought as null operator\n",
    "1. Explain what is shown in <<resnet_surface>>.<br>\n",
    "    loss values wrt to wrt to params of the network. The resnet smmothen the curve.\n",
    "1. When is top-5 accuracy a better metric than top-1 accuracy?<br>\n",
    "    A metric testing when the output have a chance of mislabeld. We can take the top 5 prediction and check if any of them matches the labels\n",
    "1. What is the \"stem\" of a CNN?<br>\n",
    "    a series of convolution which is not resnet to scale up the initial image.\n",
    "1. Why do we use plain convolutions in the CNN stem, instead of ResNet blocks?<br>\n",
    "    resnet helps us when we are deep in the network, propagation of activation is not uniform. Byusing stem initially we give the networks more params to learn.\n",
    "1. How does a bottleneck block differ from a plain ResNet block?<br>\n",
    "    it has `1*1` convolutioon before and after the main `3*3` convolution.\n",
    "1. Why is a bottleneck block faster?<br>\n",
    "    As there are many `1*1` convolution which are faster to calculate\n",
    "1. How do fully convolutional nets (and nets with adaptive pooling in general) allow for progressive resizing?<br>\n",
    "    The no of channel increases with each stride. And the final layer uses `nn.AdaptiveAvgPool2d`, extrat/generalize the input channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
